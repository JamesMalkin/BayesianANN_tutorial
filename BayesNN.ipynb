{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ba5jtUO1w0BzD-2hTB9BpXJY838j4g8z",
      "authorship_tag": "ABX9TyPoK6Q6J9xaoMrIGK+QxKRp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesMalkin/BayesianANN_tutorial/blob/main/BayesNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up dependencies and upload processed data"
      ],
      "metadata": {
        "id": "1TA5xCAbr0mu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9lbTUCfq4-w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.distributions \n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    dev = \"cuda\" \n",
        "else:  \n",
        "    dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "trainset = torchvision.datasets.MNIST(root = '.data/trainset', train = True, transform=transform, download=True)\n",
        "testset = torchvision.datasets.MNIST(root = '.data/testset', train = False, transform=transform, download=True)\n",
        "\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian neural network"
      ],
      "metadata": {
        "id": "OIqMP0kUsFZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCHSIZE = 20\n",
        "\n",
        "#trainset = torch.load('./.data/trainset/MNIST/processed/training.pt')\n",
        "#trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCHSIZE,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "TRAINING_INSTANCES = len(trainloader)\n",
        "\n",
        "#testset = torch.load('./.data/testset/MNIST/processed/test.pt')\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCHSIZE,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "TEST_INSTANCES = len(testloader)\n",
        "\n",
        "\n",
        "class NetLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Weight parameters\n",
        "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features, device=device,dtype=torch.double).uniform_(-0.1, 0.1))\n",
        "        self.weight_phi = nn.Parameter((torch.full((out_features, in_features), torch.log(torch.exp((torch.tensor(1e-4, dtype=torch.double)))-1), device=device).double()))\n",
        "        \n",
        "        # Bias parameters\n",
        "        self.bias_mu = nn.Parameter(torch.empty(out_features, device=device, dtype=torch.double).uniform_(-0.1, 0.1)) #(-0.2, 0.2) \n",
        "        self.bias_phi = nn.Parameter(torch.full((1, out_features), torch.log(torch.exp((torch.tensor(1e-4, dtype=torch.double)))-1), device=device).double()) #was 0.01 before january\n",
        "        \n",
        "    \n",
        "    def forward(self, input, sample=False):\n",
        "        weight_sig = F.softplus(self.weight_phi)\n",
        "        bias_sig = F.softplus(self.bias_phi)\n",
        "        weight_var  = torch.pow(weight_sig.detach().clone(),2)\n",
        "        bias_var  = torch.pow(bias_sig.detach().clone(),2)\n",
        "                          \n",
        "        weight_dist = torch.distributions.Normal(self.weight_mu, weight_sig)\n",
        "        bias_dist = torch.distributions.Normal(self.bias_mu, bias_sig)\n",
        "        \n",
        "        weight = weight_dist.rsample()\n",
        "        bias = bias_dist.rsample()\n",
        "\n",
        "        if sample:\n",
        "            Net.ent_loss += Net.ent_cost_func(self.weight_mu, weight_sig, weight).sum()\n",
        "            Net.ent_loss += Net.ent_cost_func(self.bias_mu, bias_sig, bias).sum()\n",
        "            Net.prior_loss += Net.prior_cost_func(weight).sum()\n",
        "            Net.prior_loss += Net.prior_cost_func(bias).sum()\n",
        "                \n",
        "        return F.linear(input, weight, bias)\n",
        "                   \n",
        "class Net(nn.Module):\n",
        "    def __init__(self, power=2, scale=1): \n",
        "        super().__init__()\n",
        "        self.firingrate = []\n",
        "        self.p = power\n",
        "        self.s = torch.tensor(scale, device=device)\n",
        "        self.linear1 = NetLayer(28*28, 100)\n",
        "        self.linear2 = NetLayer(100, 100)\n",
        "        self.linear3 = NetLayer(100, 10)\n",
        "\n",
        "    def ent_cost_func(self, sample, sig, mu):\n",
        "        return (BATCHSIZE/TRAINING_INSTANCES)*torch.sum(torch.distributions.normal.Normal(mu, sig).log_prob(sample))\n",
        "    \n",
        "    def prior_cost_func(self, sample):\n",
        "        return (BATCHSIZE/TRAINING_INSTANCES)*1*torch.sum(sample**2)\n",
        "\n",
        "        \n",
        "    def forward(self, x, sample=False, biosample=False, lang=False, noise=False, s=False, batch_idx=False, epoch=False):\n",
        "        self.prior_loss = 0\n",
        "        self.ent_loss = 0\n",
        "        self.like_loss = 0\n",
        "        \n",
        "        x = x.view(-1, 784)\n",
        "        x = self.linear1(x, sample)\n",
        "        x = F.relu(self.linear2(x, sample, biosample, lang, noise))\n",
        "        x = F.relu(self.linear3(x, sample))\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "    \n",
        "    @staticmethod\n",
        "    def loss(pred_values, true_values):\n",
        "        criterion = nn.NLLLoss(reduction='mean')\n",
        "        loss = criterion(pred_values, true_values)*BATCHSIZE*10\n",
        "        return loss"
      ],
      "metadata": {
        "id": "DHmUcwTnrv38"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}