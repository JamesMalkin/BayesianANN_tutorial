{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ba5jtUO1w0BzD-2hTB9BpXJY838j4g8z",
      "authorship_tag": "ABX9TyNTYxYSQPbCaE79li9gwI+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesMalkin/BayesianANN_tutorial/blob/main/BayesNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up dependencies and upload processed data"
      ],
      "metadata": {
        "id": "1TA5xCAbr0mu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9lbTUCfq4-w",
        "outputId": "6d3baffb-a5a9-4f1d-e161-d3eb85e129fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3560880f70>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.distributions \n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    dev = \"cuda\" \n",
        "else:  \n",
        "    dev = \"cpu\"\n",
        "print('device', dev)\n",
        "device = torch.device(dev)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "trainset = torchvision.datasets.MNIST(root = '.data/trainset', train = True, transform=transform, download=True)\n",
        "testset = torchvision.datasets.MNIST(root = '.data/testset', train = False, transform=transform, download=True)\n",
        "\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian neural network"
      ],
      "metadata": {
        "id": "OIqMP0kUsFZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCHSIZE = 20\n",
        "\n",
        "#trainset = torch.load('./.data/trainset/MNIST/processed/training.pt')\n",
        "#trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCHSIZE,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "TRAINING_INSTANCES = len(trainloader)*BATCHSIZE\n",
        "\n",
        "\n",
        "#testset = torch.load('./.data/testset/MNIST/processed/test.pt')\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCHSIZE,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "TEST_INSTANCES = len(testloader)*BATCHSIZE\n",
        "\n",
        "                   \n",
        "class Net(nn.Module):\n",
        "    def __init__(self): \n",
        "        super().__init__()\n",
        "        self.firingrate = []\n",
        "        self.linear1 = NetLayer(28*28, 100)\n",
        "        self.linear2 = NetLayer(100, 10)\n",
        "        #self.linear3 = NetLayer(100, 10)\n",
        "\n",
        "    def ent_cost_func(self, sample, sig, mu):\n",
        "        return (BATCHSIZE/TRAINING_INSTANCES)*torch.sum(torch.distributions.normal.Normal(mu, sig).log_prob(sample))\n",
        "    \n",
        "    def prior_cost_func(self, sample):\n",
        "        return (BATCHSIZE/TRAINING_INSTANCES)*1*torch.sum(sample**2)\n",
        "  \n",
        "    def forward(self, x, sample=False, biosample=False, lang=False, noise=False, s=False, batch_idx=False, epoch=False):\n",
        "        self.prior_loss = 0\n",
        "        self.ent_loss = 0\n",
        "        self.like_loss = 0\n",
        "        \n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.linear1(x, sample))\n",
        "        x = self.linear2(x, sample)\n",
        "        #x = F.relu(self.linear3(x, sample))\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "    \n",
        "    @staticmethod\n",
        "    def loss(pred_values, true_values):\n",
        "        criterion = nn.NLLLoss(reduction='mean')\n",
        "        loss = criterion(pred_values, true_values)*BATCHSIZE*10\n",
        "        return loss\n",
        "\n",
        "class NetLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Weight parameters\n",
        "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features, device=device,dtype=torch.double).uniform_(-0.1, 0.1))\n",
        "        self.weight_phi = nn.Parameter((torch.full((out_features, in_features), torch.log(torch.exp((torch.tensor(1e-4, dtype=torch.double)))-1), device=device).double()))\n",
        "        \n",
        "        # Bias parameters\n",
        "        self.bias_mu = nn.Parameter(torch.empty(out_features, device=device, dtype=torch.double).uniform_(-0.1, 0.1)) #(-0.2, 0.2) \n",
        "        self.bias_phi = nn.Parameter(torch.full((1, out_features), torch.log(torch.exp((torch.tensor(1e-4, dtype=torch.double)))-1), device=device).double()) #was 0.01 before january\n",
        "        \n",
        "    \n",
        "    def forward(self, input, sample=False):\n",
        "        weight_sig = F.softplus(self.weight_phi)\n",
        "        bias_sig = F.softplus(self.bias_phi)\n",
        "        weight_var  = torch.pow(weight_sig.detach().clone(),2)\n",
        "        bias_var  = torch.pow(bias_sig.detach().clone(),2)\n",
        "                          \n",
        "        weight_dist = torch.distributions.Normal(self.weight_mu, weight_sig)\n",
        "        bias_dist = torch.distributions.Normal(self.bias_mu, bias_sig)\n",
        "        \n",
        "        if sample:\n",
        "          weight = weight_dist.rsample()\n",
        "          bias = bias_dist.rsample()\n",
        "        else:\n",
        "          weight = self.weight_mu\n",
        "          bias = self.bias_mu\n",
        "\n",
        "        if sample:\n",
        "            net.ent_loss += net.ent_cost_func(self.weight_mu, weight_sig, weight).sum()\n",
        "            net.ent_loss += net.ent_cost_func(self.bias_mu, bias_sig, bias).sum()\n",
        "        net.prior_loss += net.prior_cost_func(weight).sum()\n",
        "        net.prior_loss += net.prior_cost_func(bias).sum()\n",
        "                \n",
        "        return F.linear(input, weight, bias)"
      ],
      "metadata": {
        "id": "DHmUcwTnrv38"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test function"
      ],
      "metadata": {
        "id": "aCi6AUQltS8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_SAMPLING = 1\n",
        "TEST_SAMPLING = 10\n",
        "\n",
        "def train(sample=False):\n",
        "    running_loss = 0\n",
        "    running_like_loss = 0\n",
        "    running_ent_loss = 0\n",
        "    running_prior_loss = 0\n",
        "\n",
        "    mode_performance_list = [] \n",
        "    mode_loss_list = []\n",
        "    sample_performance_list = [] \n",
        "    sample_loss_list = []\n",
        "    expected_performance_list = []\n",
        "    expected_loss_list = []\n",
        "\n",
        "    for epoch in range(60):\n",
        "        for batch_idx, (data, target) in enumerate(trainloader):\n",
        "            if batch_idx <= len(trainloader):\n",
        "                net.train()\n",
        "                data = data.to(device)\n",
        "                data = data.type(torch.double)\n",
        "                target = target.to(device)\n",
        "                target = target.type(torch.long)\n",
        "                net.zero_grad()\n",
        "                like_loss = 0\n",
        "                ent_loss = 0\n",
        "                prior_loss = 0\n",
        "\n",
        "                for j in range(TRAINING_SAMPLING):\n",
        "                    preds = net(data, sample=sample)\n",
        "                    like_loss += net.loss(preds, target)\n",
        "                \n",
        "                loss = like_loss + net.prior_loss + net.ent_loss\n",
        "                loss = loss/TRAINING_SAMPLING\n",
        "                loss.backward()\n",
        "                            \n",
        "                running_loss += loss.item()/TRAINING_SAMPLING\n",
        "                running_like_loss += like_loss.item()/TRAINING_SAMPLING\n",
        "                running_ent_loss += net.ent_loss.item()/TRAINING_SAMPLING\n",
        "                running_prior_loss += net.prior_loss.item()/TRAINING_SAMPLING\n",
        "               \n",
        "                if net.training:\n",
        "                    optimiser.step()\n",
        "\n",
        "                if batch_idx % TRAINING_INSTANCES/BATCHSIZE == (TRAINING_INSTANCES/BATCHSIZE)-1: # Print every so mini-batches (epoch)\n",
        "                    print('[Epoch-{}, Batch-{} total loss= {}, like_loss={}, ent_loss= {}, prior_loss= {}'.format(epoch + 1, batch_idx + 1, running_loss / (3000*BATCHSIZE), running_like_loss/(3000*BATCHSIZE), running_ent_loss/(3000*BATCHSIZE), running_prior_loss/(3000*BATCHSIZE)))\n",
        "                    running_loss = 0.0\n",
        "                    running_like_loss = 0.0\n",
        "                    running_prior_loss = 0.0\n",
        "                    running_ent_loss = 0.0\n",
        "    \n",
        "                    mode_performance, mode_loss = test(sample=False, exp_accuracy=False)\n",
        "                    sample_performance, sample_loss = test(sample=True, exp_accuracy=False)\n",
        "                    expected_performance, expected_loss = test(sample=True, exp_accuracy=True)\n",
        "\n",
        "                    mode_performance_list.append(mode_performance)\n",
        "                    mode_loss_list.append(mode_loss)\n",
        "                    sample_performance_list.append(sample_performance)\n",
        "                    sample_loss_list.append(sample_loss)\n",
        "                    expected_performance_list.append(expected_performance)\n",
        "                    expected_loss_list.append(expected_loss)\n",
        "    \n",
        "    return mode_performance_list, mode_loss_list, sample_performance_list, sample_loss_list, expected_performance_list, expected_loss_list\n",
        "\n",
        "def test(sample=False, classes=10, exp_accuracy=False):\n",
        "    correct = 0\n",
        "    loss = 0\n",
        "    net.eval()\n",
        "    if exp_accuracy:\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                preds = torch.zeros(BATCHSIZE, 10).to(device)\n",
        "                class_preds = torch.zeros(BATCHSIZE, 1)\n",
        "                for n in range(TEST_SAMPLING):\n",
        "                    images, labels = data\n",
        "                    images = images.to(device)\n",
        "                    images = images.type(torch.double) \n",
        "                    labels = labels.to(device)\n",
        "                    labels = labels.type(torch.long)\n",
        "                    preds += net(images, sample=sample)\n",
        "                    \n",
        "                preds /= TEST_SAMPLING\n",
        "                loss += net.loss(preds, labels)\n",
        "                class_preds = preds.max(1, keepdim=True)[1]\n",
        "                correct += class_preds.eq(labels.view(-1, 1)).sum().item()\n",
        "        accuracy = 100 * correct / (TEST_INSTANCES)\n",
        "        loss /= TEST_INSTANCES\n",
        "        print('Expected Accuracy', np.round(accuracy,3))\n",
        "        print('Expected Loss', np.round(loss.item(),3))\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                images, labels = data\n",
        "                images = images.to(device) \n",
        "                labels = labels.to(device)\n",
        "                images = images.type(torch.double) \n",
        "                labels = labels.type(torch.long)\n",
        "                preds = net(images, sample=sample)\n",
        "                loss += net.loss(preds, labels)\n",
        "                class_preds = preds.max(1, keepdim=True)[1]\n",
        "                correct += class_preds.eq(labels.view(-1, 1)).sum().item()\n",
        "            \n",
        "        loss = loss / TEST_INSTANCES\n",
        "        accuracy = 100 * correct / TEST_INSTANCES\n",
        "        if sample:\n",
        "          print('Sample Accuracy', np.round(accuracy,3))\n",
        "          print('Sample Loss', np.round(loss.item(),3))\n",
        "        else:\n",
        "          print('Mode Accuracy', np.round(accuracy,3))\n",
        "          print('Mode Loss', np.round(loss.item(),3))\n",
        "    return accuracy, loss\n"
      ],
      "metadata": {
        "id": "tJVCTgIRtSjD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run simulation\n"
      ],
      "metadata": {
        "id": "0Aimih6wx6Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "optimiser = optim.Adam(net.parameters(), lr=0.001)\n",
        "mode_performance_list, mode_loss_list, sample_performance_list, sample_loss_list, expected_performance_list, expected_loss_list = train(sample=True)\n",
        "\n",
        "np.save('mode_performance_list', np.array(mode_performance_list))\n",
        "np.save('mode_loss_list', np.array(mode_loss_list))\n",
        "np.save('sample_performance_list', np.array(sample_performance_list))\n",
        "np.save('mode_loss_list', np.array(mode_loss_list))\n",
        "np.save('expected_performance_list', np.array(expected_performance_list))\n",
        "np.save('expected_loss_list', np.array(expected_loss_list))"
      ],
      "metadata": {
        "id": "MUz1X1Zxx4s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the best estimate for the network, a noisy network and the average vote of a noisy network. These correspond to the mode of the posterior, single samples from the posterior, and the average of the predictive posterior, respectively."
      ],
      "metadata": {
        "id": "8hVaJqCjRgIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mode_performance, mode_loss = test(sample=False, exp_accuracy=False)\n",
        "sample_performance, sample_loss = test(sample=True, exp_accuracy=False)\n",
        "expected_performance, expected_loss = test(sample=True, exp_accuracy=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23cHiZONRge_",
        "outputId": "286e1db1-d5d9-477d-8a59-6212e7a39f8a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Accuracy 1869.4\n",
            "Sample Loss 42.309\n",
            "Sample Accuracy 1869.2\n",
            "Sample Loss 42.315\n",
            "Expected Accuracy 1869.4\n",
            "Expected Loss 42.312\n"
          ]
        }
      ]
    }
  ]
}